### Literature review on Media Biases

[Literature review 1 Source](https://www.tandfonline.com/doi/full/10.1080/21670811.2021.1969974)

**Summary**: 

The article examines how AI-generated news can mitigate perceptions of hostile media bias (HMB) through the activation of the machine heuristic, which suggests machines are more objective and unbiased than humans. Using an online experiment, it finds that stories presented as sourced from AI journalists lower HMB, particularly among individuals with more extreme partisan attitudes. The effect of machine cues on reducing perceived bias suggests potential for AI in journalism to address challenges of public distrust and political polarization.

[Literature review 2 Source](https://www.tandfonline.com/doi/full/10.1080/14680777.2023.2263659)

**Summary**: 

The study, "The Invisible Women: Uncovering Gender Bias in AI-generated Images of Professionals," explores gender bias in AI-generated images across four professions (law, medicine, engineering, and scientific research). Analyzing 99 images from nine text-to-image generators, the study finds a significant gender bias, with men represented in 76% of images and women only in 8%. This bias is consistent across all professions and varies among different AI generators, highlighting the need for more inclusive AI design to address gender inequalities.


[Literature review 3 Source](https://www.psu.edu/news/information-sciences-and-technology/story/trained-ai-models-exhibit-learned-disability-bias-ist/)

**Summary**:

The study conducted by researchers from Penn State's College of Information Sciences and Technology investigates biases against people with disabilities in natural language processing (NLP) algorithms and sentiment analysis models. Published in 2023, the paper titled "Automated Ableism" addresses the increasing use of AI-driven sentiment analysis tools and highlights concerns regarding the biases embedded in these systems. By analyzing social media conversations related to disabilities, the researchers demonstrate the presence of significant bias in sentiment and toxicity analysis models, with statements about disabilities often receiving more negative and toxic scores. The study emphasizes the need to address disability bias in AI models to mitigate social harm and promote inclusivity in AI development and deployment.

[Literature review 4 Source](https://www.mdpi.com/2073-431X/12/2/37)

**Summary**:

Artificial Intelligence and Sentiment Analysis: A Review in Competitive Research(2023) by Taherdoost et. al  present a comprehensive review of AI-based sentiment analysis in competitive research. They emphasize the growing importance of understanding customer sentiments for strategic decision-making. The review outlines the evolution of sentiment analysis methodologies, challenges faced, and future prospects in leveraging AI for competitive advantage.This paper reviews the application of artificial intelligence (AI) in sentiment analysis for competitive research. It explores how AI-powered sentiment analysis aids in understanding customer perceptions and competitive dynamics. The review covers literature from 2012 to 2022, highlighting the development, challenges, and prospects of AI-based sentiment analysis in competitive research.

[Literature review 5 Source](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8402961/)

**Summary**:  

Nandwani et. al(2021) delve into the realm of sentiment analysis and emotion detection from textual data, elucidating their pivotal roles in deciphering human behavior within diverse contexts. They underscore the burgeoning significance of social media platforms as repositories of human emotions and opinions, necessitating efficient computational methods for sentiment and emotion analysis. The authors provide insights into various methodologies employed, ranging from lexicon-based approaches to machine learning and deep learning techniques, while also highlighting the challenges inherent in dealing with contextual nuances and linguistic ambiguities. Moreover, they elucidate the intricate facets of emotion models, emphasizing the multidimensional nature of human emotions and the complexities involved in their automated recognition. Nandwani and Verma (2021) further expound on the critical steps involved in sentiment and emotion analysis, including dataset collection, preprocessing, feature extraction, model development, and evaluation. They stress the paramount importance of robust datasets in training accurate models and discuss the trade-offs between different analysis approaches, such as lexicon-based methods, machine learning algorithms, and deep learning architectures. Additionally, the review underscores the pivotal role of pre-processing and feature extraction techniques in enhancing the performance of sentiment and emotion analysis systems. By offering a comprehensive synthesis of existing methodologies, challenges, and future directions, the paper serves as a valuable resource for researchers and practitioners aiming to navigate the intricate landscape of sentiment analysis and emotion detection from textual data.

[Literature review 6 Source](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8967082/)

**Summary**:  
The study by Peters (2022) delves into the underexplored realm of algorithmic bias in artificial intelligence (AI) systems concerning individuals' political orientation. While existing research predominantly scrutinizes biases related to gender and racial identity, Peters argues that biases based on political orientation, being less constrained by societal norms, pose distinctive challenges. The paper emphasizes the increasing role of AI systems in pivotal decision-making processes, such as job recruitment or loan eligibility, and contends that biases rooted in political orientation may be harder to detect and eradicate than their gender or racial counterparts. By shedding light on the unique risks associated with algorithmic political bias, the paper calls for heightened awareness and examination within the AI community. His paper fills a crucial gap by introducing the concept of algorithmic political bias and highlighting its distinctive challenges. The author contends that, unlike gender and racial biases, there are fewer potent societal norms to restrain political biases, making them more pervasive and influential in AI algorithms. The study underlines the potential harm caused by algorithmic political biases, particularly as algorithms can now involuntarily uncover individuals' political orientations. The analysis suggests that changes in social norms regarding responses to political differences might mitigate such biases but cautions that these norms have complexities, complicating the task of addressing algorithmic political bias effectively.
